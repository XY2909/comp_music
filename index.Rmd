---
title: "Computational Musicology"
author: "Xing Yu Pan"
date: "spring 2021"
output: 
  flexdashboard::flex_dashboard:
  storyboard: true
  vertical_layout: scroll
  theme: cosmo
---

```{r setup, include=FALSE,  warning=FALSE}

library(tidyverse)
library(spotifyr)
library(plotly)
library(compmus)
```




Visualisations {.storyboard}
=========================================

### NEW! Estimating keys: Spotify has difficulty estimating keys in a less traditional track


```{r}
circshift <- function(v, n) {
  if (n == 0) v else c(tail(v, n), head(v, -n))
}

#      C     C#    D     Eb    E     F     F#    G     Ab    A     Bb    B
major_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    0,    0)
minor_chord <-
  c(   1,    0,    0,    1,    0,    0,    0,    1,    0,    0,    0,    0)
seventh_chord <-
  c(   1,    0,    0,    0,    1,    0,    0,    1,    0,    0,    1,    0)

major_key <-
  c(6.35, 2.23, 3.48, 2.33, 4.38, 4.09, 2.52, 5.19, 2.39, 3.66, 2.29, 2.88)
minor_key <-
  c(6.33, 2.68, 3.52, 5.38, 2.60, 3.53, 2.54, 4.75, 3.98, 2.69, 3.34, 3.17)

chord_templates <-
  tribble(
    ~name, ~template,
    "Gb:7", circshift(seventh_chord, 6),
    "Gb:maj", circshift(major_chord, 6),
    "Bb:min", circshift(minor_chord, 10),
    "Db:maj", circshift(major_chord, 1),
    "F:min", circshift(minor_chord, 5),
    "Ab:7", circshift(seventh_chord, 8),
    "Ab:maj", circshift(major_chord, 8),
    "C:min", circshift(minor_chord, 0),
    "Eb:7", circshift(seventh_chord, 3),
    "Eb:maj", circshift(major_chord, 3),
    "G:min", circshift(minor_chord, 7),
    "Bb:7", circshift(seventh_chord, 10),
    "Bb:maj", circshift(major_chord, 10),
    "D:min", circshift(minor_chord, 2),
    "F:7", circshift(seventh_chord, 5),
    "F:maj", circshift(major_chord, 5),
    "A:min", circshift(minor_chord, 9),
    "C:7", circshift(seventh_chord, 0),
    "C:maj", circshift(major_chord, 0),
    "E:min", circshift(minor_chord, 4),
    "G:7", circshift(seventh_chord, 7),
    "G:maj", circshift(major_chord, 7),
    "B:min", circshift(minor_chord, 11),
    "D:7", circshift(seventh_chord, 2),
    "D:maj", circshift(major_chord, 2),
    "F#:min", circshift(minor_chord, 6),
    "A:7", circshift(seventh_chord, 9),
    "A:maj", circshift(major_chord, 9),
    "C#:min", circshift(minor_chord, 1),
    "E:7", circshift(seventh_chord, 4),
    "E:maj", circshift(major_chord, 4),
    "G#:min", circshift(minor_chord, 8),
    "B:7", circshift(seventh_chord, 11),
    "B:maj", circshift(major_chord, 11),
    "D#:min", circshift(minor_chord, 3)
  )

key_templates <-
  tribble(
    ~name, ~template,
    "Gb:maj", circshift(major_key, 6),
    "Bb:min", circshift(minor_key, 10),
    "Db:maj", circshift(major_key, 1),
    "F:min", circshift(minor_key, 5),
    "Ab:maj", circshift(major_key, 8),
    "C:min", circshift(minor_key, 0),
    "Eb:maj", circshift(major_key, 3),
    "G:min", circshift(minor_key, 7),
    "Bb:maj", circshift(major_key, 10),
    "D:min", circshift(minor_key, 2),
    "F:maj", circshift(major_key, 5),
    "A:min", circshift(minor_key, 9),
    "C:maj", circshift(major_key, 0),
    "E:min", circshift(minor_key, 4),
    "G:maj", circshift(major_key, 7),
    "B:min", circshift(minor_key, 11),
    "D:maj", circshift(major_key, 2),
    "F#:min", circshift(minor_key, 6),
    "A:maj", circshift(major_key, 9),
    "C#:min", circshift(minor_key, 1),
    "E:maj", circshift(major_key, 4),
    "G#:min", circshift(minor_key, 8),
    "B:maj", circshift(major_key, 11),
    "D#:min", circshift(minor_key, 3)
  )
```

```{r}
time2 <-
  get_tidy_audio_analysis("5TXY9UWJMiMopFTpW0uUBb") %>%
  compmus_align(sections, segments) %>%
  select(sections) %>%
  unnest(sections ) %>%
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "mean", norm = "manhattan"
      )
  )
```
```{r}
time2 %>% 
  compmus_match_pitch_template(
    key_templates,         # Change to chord_templates if descired
    method = "manhattan",     # Try different distance metrics
    norm = "manhattan"     # Try different norms
  ) %>%
  ggplot(
    aes(x = start + duration / 2, width = duration, y = name, fill = d)
  ) +
  geom_tile() +
  scale_fill_viridis_c(guide = "none") +
  theme_minimal() +
  labs(x = "Time (s)", y = "")

```

*** 

This chordogram is based on the same track 'Time' by Hans Zimmer as it is also being analyzed using cepstrogram from previous time. I was wondering if analyzing this track using chordogram could give me more insight into the overall structure of this iconic track. 

  First of all, we need to dive a bit deeper into the chordogram itself. The chordogram indicates the similarity values between the chroma vectors and the given chord templates/key template. Here key template is used and keys can be estimated by the chordogram. As indicated by the generated chordogram, it is interesting that in the first section (until approximate 25s) all the keys are marked with yellow, this also applies to the last section of the track (from approximate 280s and onward). This means there is a greater certainty to estimate keys in the beginning and the end of the track, according to the key templates. A possible explanation could be that this track is partly being synthesized (the middle part), as Hans Zimmer is known for integrating electronic music sounds with more traditional arrangements. This way Spotify API could have a difficult time estimating the key profiles when the sound is synthesized and thus not have a clear structure. 
  
  Generally, still some repeating patterns are found when looking at the chordogram. It can be concluded from the parts that are clearly visible, no specific keys are preferred, as all keys across the spectrum are more or less being recognized. 



### Compare the overall energy using violinplot


```{r, echo=FALSE, warning=FALSE }
Joe <- get_artist_audio_features("7nzSoJISlVJsn7O0yTeMOB")
Hans <- get_artist_audio_features("0YC192cP3KPCRWx8zr8MfZ")

```

```{r, echo=FALSE,  warning=FALSE}
awards <-
  bind_rows(
    Joe %>% mutate(category = "Joe"),
    Hans %>% mutate(category = "Hans")
  )

```
```{r, echo=FALSE,  warning=FALSE}
box <- awards %>%
  ggplot(aes(x = category, y = energy, fill = category)) +
   geom_violin() + theme(legend.position = 'none') + ylab("Overall energy")  + stat_summary(fun.y=median, geom="point", size=2, color="red")



# make it interactive
ggplotly(box)

```

***
As shown in the violinplot, both artists have a relatively low energy in terms of intensity and activity of their audio features. The graph indicates that both artists have a pretty similar energy level in terms of their work looking at the shape of the violin, this is also supported by the fact that they both have an average energy level at 0.21 (indicated by the red dot).To have a better understanding in the meaning of the feature 'energy', we introduce another feature 'valence' in the next graph. 

### Energy vs valence: a scatterplot comparison


```{r, echo=FALSE,  warning=FALSE}
p <- ggplot(awards, aes(x = valence, y = energy, color = category)) + geom_point(position = "jitter",  alpha = 0.7) + scale_x_continuous("valence") +  scale_color_discrete("Artist") 

p + geom_hline(yintercept=0.50, linetype="dashed", color = "red") + geom_vline(xintercept = 0.5,  linetype="dashed", color = "red") + annotate("text", x = 0.0, y = 1, label = "Angry", size = 3.5) + annotate("text", x = 1, y = 1, label = "Happy", size = 3.5) + annotate("text", x = 1, y = 0.0, label = "Relax", size = 3.5) + annotate("text", x = 0, y = 0, label = "Sad", size = 3.5)

```

***

In this graph valence is plotted against the energy of both artists. Each point indicates an album of artist. As we can see from the scatterplot, the density is a largest in the left-under area where both energy and valence is low. Hans Zimmer's work mostly has a very low valence level(0-0.125) and a low to medium energy level(0-0.50), while Joe Hisaishi's work has a more even contribution across the left-under section. 
From both graph we can conclude that both artist have a pretty similar composing style where the valence and the energy is low, i.e. the soundtracks sound pretty sad according to the music theory (- valence, - energy). 

### Analysing audio using chromagram: Hans Zimmer
```{r}
happy_hans <-
  get_tidy_audio_analysis("1PykJIrOF4sMsefkaF0Ttb") %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)

#1PykJIrOF4sMsefkaF0Ttb   6rvIP7Cjx0eirVTngB2VNu
happy_hans %>%
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) %>%
  compmus_gather_chroma() %>% 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  scale_fill_viridis_c()
```

*** 

As mentioned before, Hans and Joe have pretty similar composing styles according to Spotify API. I'm curious to see where the difference lies when I compare two soundtracks of them where the energy level and the valence level is the same. I'll compare Hans Zimmer's soundtrack 'Toupee Or Not- Toupee' with Joe Hisaishi's soundtrack 'The Dorok Army Stikes Back' (both have an energy of 0.73 and valence of 0.97). 

As shown in the chromagram, this soundtrack consist of the use of various keys, pitches with the most energy are mostly in the range of F# to A. Also pitch B is commonly used. The green block (pitch B) at the end of the timeline could be explained as the soundtrack is ended with a crash cymbal.

### Analysing audio using chromagram: Joe Hisaishi
```{r}
happy_joe <-
  get_tidy_audio_analysis("0bbLyGASEHwDVUm5KOccIc") %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)

happy_joe %>%
  mutate(pitches = map(pitches, compmus_normalise, "euclidean")) %>%
  compmus_gather_chroma() %>% 
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = pitch_class,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  theme_minimal() +
  scale_fill_viridis_c()
```

*** 
Compared to the previous chromagram of Zimmer's soundtrack, Hisaishi's soundtrack has the most energy in pitch G over time. Differ from Zimmer, this soundtrack pay little attention to pitch B although it's easier to see what they do have in common than what they don't.



### Building up intensity: Hans Zimmer's piece of epicness 

```{r}
time <-  
  get_tidy_audio_analysis("5TXY9UWJMiMopFTpW0uUBb") %>%     # Change URI.
  compmus_align(bars, segments) %>%                      # Change `bars`
  select(bars) %>%                                      #   in all three
  unnest(bars) %>%                                      #   of these lines.
  mutate(
    pitches =
      map(segments,
        compmus_summarise, pitches,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  ) %>%
  mutate(
    timbre =
      map(segments,
        compmus_summarise, timbre,
        method = "rms", norm = "euclidean"              # Change summary & norm.
      )
  )

time %>%
  compmus_gather_timbre() %>%
  ggplot(
    aes(
      x = start + duration / 2,
      width = duration,
      y = basis,
      fill = value
    )
  ) +
  geom_tile() +
  labs(x = "Time (s)", y = NULL, fill = "Magnitude") +
  scale_fill_viridis_c() +                              
  theme_classic()
```

*** 

This cepstrogram is based on Hans Zimmer's soundtrack 'Time' which he provide for the ending sequence of the film 'Inception'. The structure of this track can be seen in the cepstrogram in which c01 is the Spotify timbre component explaining the loudness, c02 the amount of energy in the lower frequency and c03 the amount of energy in the midrange frequency of a given timeframe. It is clear that the loudness of the first half (up to approx. 120 sec) is slowing building up, with instruments playing like piano, cello and tuba, providing energy in the low and midrange frequencies. As the loudness builds up, the intensity of the track also builds up, then from 120 sec to approximately 220 sec the climax is reached, which is also the part with intertwining melodies that we are all so familiar to. In this part layers of instruments take part, mimicking the narrative of the film where the emotion of the protagonist reached the top as he thinks about his wife back in limbo. This could also explain the energy in more abstract layers in the cepstrogram (from c04 and up) as more instruments intertwining with each other. Afterwards the loudness drops as a sign of fade out while instruments with low frequencies still plays (as seen in the last part of c02).  

  
Introduction 
=========================================


### Overview Corpus 

*** PLEASE DO NOT SHOW THIS PORTFOLIO IN CLASS ***

For this portfolio I choose to compare two of my favorite composers: Joe Hisaishi and Hans Zimmer. They are both know for their scores in the film industry with their own unique styles. I'm interested in how they differ in the way they compose and how this can be explained by the geographic region, as Joe Hisaishi mostly works with Japanese films and Hans Zimmer with his career in Hollywood. 

### Short Introduction of the composers

As Joe Hisaishi is one of the most famous Japanese composer, known for over 100 film scores and the association with the animator Hayao Miyazaki, he has provide many composition which has worldwide success, including the opening theme of Miyazaki's film Spirited Away and later on the score of Howl's Moving Castle. His composingstyle can be described as gentle, minimalistic and melancholic, which also becomes a trademark of much of the studio Ghibli's output. 

Hans Zimmer is a German film score composer and producer. His work are notable for integrating electronic sounds with traditional orchestral arrangements. He has many award winning compositions include The Lion King and the Pirates of the Caribbean series. 



Discussion
=========================================

### About the chromagram

Chromagrams is handy in the way you can align two pieces together and tell the difference, but it's pretty difficult in this case, as two composers don't have a piece where they both have worked on. So the logical next step is using chromagram to annotate key moments as you can see the energy levels of various pitches. However the compared pieces are totally different and it's hard to tell whether chromagram is a good choice here. 






